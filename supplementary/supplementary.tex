\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

\DeclareMathOperator*{\argmax}{arg\,max}

\title{Unified variation discovery and genotyping from high throughput sequencing data: supplementary material}
\author{}
\date{}

\begin{document}

\maketitle

This document describes in detail the algorithms and models implemented in the variant calling software Octopus.

\tableofcontents

\section{Introduction}

\section{Methods}

\subsection{Read QC}

\subsection{Candidate allele generation}

\subsubsection{Raw CIGAR alignment}

\subsubsection{Local re-assembly}

\subsection{Haplotype generation}

\subsection{Haplotype likelihood calculation}

\subsubsection{Kmer based re-mapping}

\subsubsection{Read error modelling}

\subsubsection{Pair Hidden Markov Model evaluation}

\subsection{Haplotype filtering}

\subsection{Calling models}

Although each genotype model is in a sense independent, they do share common attributes which we define here for brevity.

\begin{center}
\begin{tabular}{ll}
Constants & Description \\
\hline
$P_s$ & The ploidy of organism $s$\\
$h$ & A haplotype \\
$\mathbb{M_{e}}$ & The sequencing error model \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Latent variables & Description \\
\hline
$r$ & A sequencing read \\
\hline
\end{tabular}
\end{center}

Then the conditional probability of a read given a haplotype is:

\begin{equation}
\label{eq:read_prob}
    p(r | h, \mathbb{M_{e}})
\end{equation}

Which has already been described. For brevity, we will from here on omit the $\mathbb{M_{e}}$ term and just write $p(r | h)$.

\subsubsection{Coalescent mutation model}

\begin{equation*}
    p(\boldsymbol{g} | \mathcal{M}_{coal}) = \binom{\theta_1}{\theta_1 + \theta_2}^{k_1} \binom{\theta_2}{\theta_1 + \theta_2}^{k_2} \binom{k_1 + k_2}{k_1} p_{\theta = \theta_1 + \theta_2} (k_1 + k_2)
\end{equation*}

Where $p_\theta(S = k)\sum_{i=2}^n (-1)^i \binom{n - 1}{i - 1} \binom{i - 1}{\theta + i - 1} \binom{\theta}{\theta + i - 1}^k$

\subsubsection{Somatic mutation prior model}

\subsubsection{De novo mutation prior model}

\subsubsection{Individual model}

This model is designed to model reads coming from a single individual with a known fixed ploidy. It is the simplest genotype model that Octopus offers, and is also the fastest to compute. It is used by multiple Octopus variant callers.

\subsubsection{Definitions}

\begin{center}
\begin{tabular}{ll}
Constants & Description \\
\hline
$P$ & The individuals ploidy \\
$G$ & The number of genotypes \\
$\boldsymbol{\phi}$ & Vector of $G$ probabilities \\
$h_{ij}$ & The $j^{th}$ haplotype of the $i^{th}$ genotype \\
$\mathcal{R}$ & The set of sequencing reads for the individual \\
$N$ & $\mathcal{R}$ \\
\hline
\end{tabular}
\end{center}

Note that $h_{ij}$ is technically a deterministic surjective function $h_{ij} = f(\boldsymbol{g}, i, j)$ which maps genotypes to haplotypes, but for brevity we just write $h_{ij}$.

\begin{center}
\begin{tabular}{ll}
Observed latent variables & Description \\
\hline
$\mathcal{R}$ & The set of sequencing reads for the individual \\
$N$ & $\mathcal{R}$ \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Hidden latent variables & Description \\
\hline
$\boldsymbol{g}$ & Binary (1-of-$G$) \\
\hline
\end{tabular}
\end{center}

\subsubsection{Marginal distributions}

The marginal distribution for $\boldsymbol{g}$ is categorical:

\begin{equation}
\label{eq:ind_g_marginal}
    p(\boldsymbol{g} | \boldsymbol{\phi}) = \prod_{i = 1}^G \phi_i^{g_i}
\end{equation}

The marginal distribution for a single read $r$ is a mixture distribution, which follows from the assumption that the haplotypes that make up a known ploidy genotype are exchangeable, and thus we must assign them equal probabilities of being sequenced:

\begin{equation}
\label{eq:ind_read_marginal}
 p(r | g_i = 1) = \frac{1}{P} \sum^P_{k = 1} p(r | h_{ik})
\end{equation}

which can also be written as:

\begin{equation}
\label{eq:ind_read_marginal2}
 p(r | \boldsymbol{g}) = \prod_{i = 1}^G \frac{1}{P} \sum^P_{k = 1} p(r | h_{ik})^{g_i}
\end{equation}

\subsubsection{Joint distribution}

\begin{equation}
\label{eq:ind_jp}
 p(\mathcal{R}, \boldsymbol{g}) = \prod_{i = 1}^G \phi_i^{g_i} \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})^{g_i}
\end{equation}

\subsubsection{Posterior distribution}

\begin{equation}
\label{eq:ind_post}
 p(\boldsymbol{g} | \mathcal{R}) = \frac{\prod_{i = 1}^G \phi_i \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})^{g_i}}{\sum_{\boldsymbol{g}'}\prod_{i = 1}^G \phi_i \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})^{g_i}}
\end{equation}

We can factor out the constant $P$ term, and write more compactly as:

\begin{equation}
\label{eq:ind_post2}
 p(g_i = 1 | \mathcal{R}) = \frac{\phi_i \prod^N_{n = 1} \sum^P_{k = 1} p(r_n | h_{ik})}{\sum_{j = 1}^G \phi_i \prod^N_{n = 1} \sum^P_{k = 1} p(r_n | h_{ik})}
\end{equation}

\subsubsection{Evidence}

The evidence for the model is simply the denominator of (\ref{eq:ind_post}):

\begin{equation}
\label{eq:ind_ev}
 p(\mathcal{R}) = \sum_{i = 1}^G \phi_i \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})
\end{equation}

\subsubsection{Population genotype model}

\subsubsection{Trio genotype model}

\subsubsection{CNV genotype model}

The copy-number-variant genotype model attempts to model copy number changes in a single individual from which multiple samples have been taken. This model can be contrasted with the individual model which assumes the haplotypes in the individuals germline genotype are present in a $1:1$ ratio (for diploid cases).

\subsubsection{Definitions}

\begin{center}
\begin{tabular}{ll}
Constants & Description \\
\hline
$P$ & The individuals germline ploidy \\
$S$ & The number of samples for the individual \\
$G$ & The number of genotypes \\
$\boldsymbol{\phi}$ & Vector of $G$ probabilities \\
$h_{ij}$ & The $j^{th}$ haplotype of the $i^{th}$ genotype \\
$\boldsymbol{\alpha_s}$ & A vector of $P$ Dirichlet counts \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Observed latent variables & Description \\
\hline
$\mathcal{R}_s$ & The set of sequencing reads for sample $s$ of the individual \\
$N_s$ & $\mathcal{R}_s$ \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Hidden latent variables & Description \\
\hline
$\boldsymbol{g}$ & Binary (1-of-$G$) \\
$\boldsymbol{\pi}_s$ & A vector of $P$ probabilities of sequencing each haplotype in the individuals genotype \\
$\boldsymbol{z}_{sn}$ & Binary (1-of-$P$), specifying which haplotype was sequenced in read $n$ of sample $s$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Marginal distributions}

The marginal distribution for $\boldsymbol{g}$ is categorical:

\begin{equation}
\label{eq:cnv_g_marginal}
    p(\boldsymbol{g} | \boldsymbol{\phi}) = \prod_{i = 1}^G \phi_i^{g_i}
\end{equation}

The marginal distribution of $\phi$ is assumed to be Dirichlet, this is mostly because it simplifies the mathematics.

\begin{equation}
\label{eq:cnv_pi_marginal}
    p(\boldsymbol{\pi} | \boldsymbol{\alpha}) = \text{Dir}(\boldsymbol{\pi} | \boldsymbol{\alpha}) = \frac{1}{\text{B}(\boldsymbol{\alpha})} \prod_{k = 1}^P \pi_k^{\alpha_k - 1}
\end{equation}

where $\text{B}(\boldsymbol{\alpha})$ is the multivariate Beta function:

\begin{equation}
\label{eq:beta_func}
    \text{B}(\boldsymbol{\alpha}) = \frac{\prod_{k = 1}^K \Gamma(\alpha_k)}{\Gamma(\sum_{k = 1}^K \alpha_k)}
\end{equation}

The marginal distribution for each $\boldsymbol{z}$ is categorical:

\begin{equation}
\label{eq:cnv_z_marginal}
    p(\boldsymbol{z} | \boldsymbol{\pi}) = \prod_{k = 1}^P \pi_k^{z_k}
\end{equation}

The marginal distribution for a single read $r$ is a mixture distribution:

\begin{align}
\label{eq:cnv_read_marginal}
 p(r | \boldsymbol{\pi}, g_i = 1) &= \sum_{\boldsymbol{z}} \sum^P_{k = 1} p(\boldsymbol{z} | \boldsymbol{\pi}) p(r | \boldsymbol{z}, h_{ik}) \\
 &= \sum^P_{k = 1} \pi_k p(r | h_{ik})
\end{align}

\subsubsection{Joint distribution}

\begin{align}
\label{eq:cnv_jp}
 p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi}) &= p(\boldsymbol{g} | \boldsymbol{\phi}) \prod_{s = 1}^S p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s) p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s) p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g}) \\
 &= \prod^G_{i = 1} \phi_i^{g_i} \prod_{s = 1}^S \frac{1}{\text{B}(\boldsymbol{\alpha}_s)} \prod_{k = 1}^P \pi_{sk}^{\alpha_{sk} - 1} \prod_{n = 1}^{N_s} \prod_{k = 1}^P \pi_{sk}^{z_{snk}} \prod^G_{i = 1} \prod_{n = 1}^{N_s} \prod_{k = 1}^P p(r_{sn} | h_{ik})^{g_i z_{snk}}
\end{align}

\subsubsection{Posterior distribution}

We are interested in the posterior distributions of $\boldsymbol{g}$ and $\boldsymbol{\pi}$, which due to the hidden latent variables $\boldsymbol{z}$ cannot be evaluated in closed form, we therefore resort to approximations. We choose a Variational Bayes approximation because it is deterministic and faster to compute than Monte Carlo based methods. A description of Variational Bayes is given in the appendix.

To evaluate approximate posterior distributions in the Variational Bayes framework we need to factorise (\ref{eq:cnv_jp}) into independent factors, there is only one possibility here:

\begin{equation}
\label{eq:cnv_vb_factors}
    q(\boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi}) = q(\boldsymbol{g}) \prod_{s = 1}^S q(\boldsymbol{Z}_s) q(\boldsymbol{\pi}_s)
\end{equation}

where non-latent variables have been omitted for brevity. We evaluate the optimal factors in tern.

\begin{align}
\label{eq:cnv_ln_q_z}
\ln q^*(\boldsymbol{Z}_s) &= \mathbb{E}_{\boldsymbol{g}, \boldsymbol{\pi}, \boldsymbol{Z}_{s' \ne s}} [\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] + \text{const} \\
&= \mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] + \mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] + \text{const} \\
&= \sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \mathbb{E}[\ln \pi_{sk}] + \sum_{n = 1}^{N_s} \sum_{k = 1}^P \sum_{i = 1}^G \mathbb{E}[g_i] \ln p(r_{sn} | h_{ik})^{g_i z_{snk}} + \text{const} \\
&= \sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \ln \rho_{snk} + \text{const}
\end{align}

where we have defined

\begin{equation}
\label{eq:cnv_ln_rho}
\ln \rho_{snk} = \ln \tilde{\pi}_{sk} + \sum_{i = 1}^G \phi_i \ln p(r_{sn} | h_{ik})
\end{equation}

and

\begin{equation}
\label{eq:cnv_ex_ln_pi}
\ln \tilde{\pi}_{sk} = \psi(\alpha_{sk}) - \psi(\hat{\alpha}_s)
\end{equation}

where $\psi$ is the digamma function and $\hat{\alpha}_{s} = \sum_{k = 1}^P \alpha_{sk}$. Exponentiating both sides of (\ref{eq:cnv_ln_q_z}) and normalising we obtain:

\begin{equation}
\label{eq:cnv_q_z}
q^*(\boldsymbol{Z}_s) = \prod_{n = 1}^{N_s} \prod_{k = 1}^P \tau_{snk}^{z_{snk}}
\end{equation}

where

\begin{equation}
\label{eq:cnv_tau}
\tau_{snk} = \frac{\rho_{snk}}{\sum_{j = 1}^P \rho_{snj}}
\end{equation}

Next we look at each $q^*(\boldsymbol{\pi}_s)$:

\begin{align}
\label{eq:cnv_ln_q_pi}
\ln q^*(\boldsymbol{\pi}_s) &= \mathbb{E}_{\boldsymbol{g}, \boldsymbol{\pi}_s' \ne s}, \boldsymbol{Z} [\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] + \text{const} \\
&= \mathbb{E}[\ln p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s)] + \mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] + \text{const} \\
&= \sum_{k = 1}^P (\alpha_k - 1) \ln \pi_{sk} + \sum_{k = 1}^P \sum_{n = 1}^{N_s} \tau_{snk} \ln \pi_{sk} + \text{const}
\end{align}

and therefore

\begin{align}
\label{eq:cnv_q_pi}
q^*(\boldsymbol{\pi}_s) &= \prod_{k = 1}^P \pi_{sk}^{\alpha_{sk} - 1} \prod_{k = 1}^P \pi_{sk}^{\sum_{n = 1}^{N_s} \tau_{snk}} \\
&=  \prod_{k = 1}^P \pi_{sk}^{\alpha_{sk} + \sum_{n = 1}^{N_s} \tau_{snk} - 1}
\end{align}

Which we can see from inspection is another Dirichlet distribution with pseudo-counts:

\begin{equation}
\label{eq:cnv_new_counts}
\alpha_{sk}^{post} = \alpha_{sk}^{prior} + \hat{N}_{sk}
\end{equation}

where $\hat{N}_{sk} = \sum_{n = 1}^{N_s} \tau_{snk}$. Finally we evaluate $q^*(\boldsymbol{g})$:

\begin{align}
\label{eq:cnv_ln_q_g}
\ln q^*(\boldsymbol{g}) &= \mathbb{E}_{\boldsymbol{\pi}, \boldsymbol{Z}} [\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] + \text{const} \\
&= \mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] + \mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] + \text{const} \\
&= \mathbb{E} \left [\sum_{i = 1}^G g_i \ln \phi_i \right] + \mathbb{E} \left[ \sum_{n = 1}^{N_s} \sum_{k = 1}^P \sum_{i = 1}^G g_i z_{snk} \ln p(r_{sn} | h_{ik}) \right] + \text{const} \\
&= \sum_{i = 1}^G \mathbb{E}[g_i] \ln \phi_i +  \sum_{i = 1}^G \mathbb{E}[g_i] \sum_{n = 1}^{N_s} \sum_{k = 1}^P \mathbb{E}[z_{snk}] \ln p(r_{sn} | h_{ik}) + \text{const} \\
&= \sum_{i = 1}^G \mathbb{E}[g_i] \left\{ \ln \phi_i + \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln p(r_{sn} | h_{ik}) \right\} + \text{const}
\end{align}

So we immediately see that

\begin{equation}
\label{eq:cnv_q_g}
q^*(\boldsymbol{g}) \propto \prod_{i = 1}^G \theta^{g_i}
\end{equation}

where

\begin{equation}
\label{eq:cnv_theta}
\ln \theta_i = \ln \phi_i + \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln p(r_{sn} | h_{ik})
\end{equation}

\subsubsection{Evidence}

The Variational Bayes framework also gives a lower-bound on the evidence for the full posterior distribution. The lower bound is given by:

\begin{align}
\label{eq:cnv_evidence}
\begin{split}
\mathcal{L} &= \mathbb{E}[\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] - \mathbb{E} [\ln q^*(\boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi})] \\
&= \mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] + \sum_{s = 1}^S \left\{ \mathbb{E}[\ln p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s)] + \mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] + \mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] \right\} \notag \\
     &\phantom{{}=\mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] } - \mathbb{E}[\ln q^*(\boldsymbol{g})] - \sum_{s = 1}^S \left\{ \mathbb{E}[\ln q^*(\boldsymbol{\pi}_s) + \mathbb{E}[\ln q^*(\boldsymbol{Z}_s)] \right\}
\end{split}
\end{align}

Where each expectation is taken with respect to $q^*$. Evaluating each term separately

\begin{equation}
\mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] = \mathbb{E}\left[\sum_{i = 1}^P g_i \ln \phi_i\right] = \sum_{i = 1}^P \mathbb{E}[g_i] \ln \phi_i = \sum_{i = 1}^P \phi_i^{post} \ln \phi_i^{prior}
\end{equation}

\begin{align}
\mathbb{E}[\ln p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s)] &= \mathbb{E}\left[\sum_{k = 1}^P (\alpha_k^{prior} - 1) \ln \pi_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{prior})\right] \notag \\ 
&= \sum_{k = 1}^P (\alpha_k^{prior} - 1) \mathbb{E} [\ln \pi_{sk}] - \ln \text{B}(\boldsymbol{\alpha}^{prior}) \notag \\
 &= \sum_{k = 1}^P (\alpha_k^{prior} - 1) \ln \tilde{\pi}_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{prior})
\end{align}

\begin{equation}
\mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] = \mathbb{E}\left[\sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \ln \pi_{sk} \right] 
= \sum_{n = 1}^{N_s} \sum_{k = 1}^P \mathbb{E}[z_{snk} \ln \pi_{sk}]
= \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln \tilde{\pi}_{sk}
\end{equation}

as $\tau$ and $z$ are independent under $q$.

\begin{equation}
\mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] = \mathbb{E}\left[ \sum_{i = 1}^G \sum_{n = 1}^{N_s} \sum_{k = 1}^P g_i z_{snk} \ln p(r_{sn} | h_{ik}) \right] = \sum_{i = 1}^G \phi_i^{post} \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln p(r_{sn} | h_{ik})
\end{equation}

\begin{equation}
\mathbb{E}[\ln q^*(\boldsymbol{g})] = \mathbb{E}\left[\sum_{i = 1}^P g_i \ln \phi_i\right] = \sum_{i = 1}^P \mathbb{E}[g_i] \ln \phi_i = \sum_{i = 1}^P \phi_i^{post} \ln \phi_i^{post}
\end{equation}

\begin{align}
\mathbb{E}[\ln q^*(\boldsymbol{\pi}_s)] =& \mathbb{E}\left[\sum_{k = 1}^P (\alpha_k^{post} - 1) \ln \pi_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{post})\right] \notag \\ 
&= \sum_{k = 1}^P (\alpha_k^{post} - 1) \mathbb{E} [\ln \pi_{sk}] - \ln \text{B}(\boldsymbol{\alpha}^{post}) \notag \\
 &= \sum_{k = 1}^P (\alpha_k^{post} - 1) \ln \tilde{\pi}_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{post})
\end{align}

\begin{equation}
\mathbb{E}[\ln q^*(\boldsymbol{Z}_s)] = \mathbb{E}\left[\sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \ln \tau_{snk} \right] = \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln \tau_{snk}
\end{equation}

These equations can also be used as a test of correctness of the implementation, as $\mathcal{L}$ should increase on each iteration.

\subsubsection{Somatic genotype model}

The somatic genotype model is identical to the CNV model, with a slight modification to the sample space. Specifically the genotype variable $\boldsymbol{g}$ from the CNV model is replaced with a 'somatic' genotype $\tilde{\boldsymbol{g}}$ which is simply a $1$-of-$P + 1$ binary random variable. A further restriction of the sample space $\tilde{\boldsymbol{g}}$ is that $h_{i(P + 1)} \ne h_{ij} \forall j \le P$, which simply states that the last (somatic) component always contains a novel haplotype (i.e. not present in the germline).

\subsubsection{Bacterial genotype model}

\subsection{Variant callers}

\subsubsection{Individual}

\subsubsection{Population}

\subsubsection{Cancer}

The cancer caller is designed to detect somatic mutations in a set of tumour samples from a single individual. The set of samples may also include a single normal sample, which is assumed to be tumour free.

\subsubsection{Pre-processing}

A set of candidate germline and 'cancer' genotypes is generated from the set of candidate haplotypes. If the number of haplotypes is strictly greater than the organism ploidy then each germline genotype will be present in the set of cancer genotypes at-least once. 

Each set of genotypes may then be filtered to reduce the complexity of the model fitting step.

\subsubsection{Model comparison}

We then fit the individual $\mathcal{M}_g$, CNV $\mathcal{M}_c$, and somatic $\mathcal{M}_s$ models to the data. For the individual case all reads are pooled together, which as can be seen from \ref{eq:ind_jp} results in the same likelihood function as treating each sample separately. We then calculate the posterior probability of each model by evaluating Bayes theorem:

\begin{equation}
\label{eq:bayes_model}
p(\mathcal{M} | \mathcal{D}) = \frac{p(\mathcal{M})p(\mathcal{D} | \mathcal{M})}{p(\mathcal{D})}
\end{equation}

In particular the term $p(\mathcal{D} | \mathcal{M})$  is the evidence the model $\mathcal{M}$, and $p(\mathcal{M})$ is the prior probability.

\subsubsection{Germline genotype calling}

The probability of each germline genotype is then:

\begin{equation}
\label{eq:som_germline_genotype}
p(\boldsymbol{g} | \mathcal{R}) = p(\mathcal{M}_g | \mathcal{R}) p(\boldsymbol{g} | \mathcal{M}_g) + p(\mathcal{M}_c | \mathcal{R}) p(\boldsymbol{g} | \mathcal{M}_c) + p(\mathcal{M}_s | \mathcal{R}) p(\boldsymbol{g} | \mathcal{M}_s)
\end{equation}

\noindent where $p(\boldsymbol{g} | \mathcal{M}_s) = \sum_{\tilde{\boldsymbol{g}} s.t. \boldsymbol{g} \in \tilde{\boldsymbol{g}}} p(\tilde{\boldsymbol{g}} | \mathcal{M}_s)$.

The called genotype is then the MAP genotype according to this posterior distribution.

\subsubsection{Germline variant calling}

The posterior probability of each candidate variant allele $a$ is then:

\begin{equation}
\label{eq:som_germline_candidate}
p(a | \mathcal{R}) = \sum_{\boldsymbol{g} s.t. a \in \boldsymbol{g}} p(\boldsymbol{g} | \mathcal{R})
\end{equation}

Germline candidates are called if the posterior is above a user defined threshold, and if the candidate is present in the called germline genotype. If the candidate is not above the user-defined threshold, it is added to a list of candidate somatic candidates.

\subsubsection{Somatic allele calling}

Noting that $\mathcal{M}_s$ defines a probability distribution of probabilities over each component of each samples 'cancer genotype', we define the probability that a particular sample contains a somatic haplotype, $\tilde{h}$, as:

\begin{equation}
\label{eq:somatic_probability_sample}
p(\tilde{h}_s | \mathcal{M}_s) = p(\tilde{h}_s > c_s | \mathcal{M}_s) = \int_c^1 \text{Beta}(\alpha_{P + 1}, \sum_{i = 0}^{P} \alpha_i)
\end{equation}

\noindent where $c_s$ is some user-defined threshold. The idea being somatic haplotypes with most probability mass less than $c_s$ are considered noise.

We can then calculate the probability that a somatic haplotype is present in any of the samples as:

\begin{equation}
\label{eq:somatic_probability}
p(\tilde{h} | \mathcal{M}_s) = 1 - \prod_s 1 - p(\tilde{h}_s | \mathcal{M}_s)
\end{equation}

\noindent If this probability is greater than some user-defined threshold, we proceed to call somatic mutations.

\subsection{Local phasing of called sites}

\subsection{Variant filtering}

\subsection{VCF output}

\section{Results}

\subsection{Germline variants from WGS}

\subsubsection{Data}

Whole genome germline calls were made using sequencing data derived from the well studied individual NA12878. We found four sequencing libraries for NA12878, three high coverage and one low coverage, for evaluation:

\begin{enumerate}
    \item High coverage Illumina reads from the 1000G phase 3 project, Aligned reads were downloaded from \url{ftp.1000genomes.ebi.ac.uk:/vol1/ftp/phase3/data/NA12878/high_coverage_alignment/NA12878.mapped.ILLUMINA.bwa.CEU.high_coverage_pcr_free.20130906.bam}.
    \item Low coverage Illumina reads from the 1000G phase 3 project. Aligned reads were downloaded from \url{ftp.1000genomes.ebi.ac.uk:/vol1/ftp/phase3/data/NA12878/alignment/NA12878.mapped.ILLUMINA.bwa.CEU.low_coverage.20121211.bam}.
    \item High coverage Illumina reads from the Illumina Platinum Genomes project. Raw FASTQ files were downloaded from \url{https://storage.cloud.google.com/genomics-public-data/platinum-genomes/fastq/ERR194147_1.fastq.gz?_ga=2.121231755.-2019438195.1508521573} and \url{https://storage.cloud.google.com/genomics-public-data/platinum-genomes/fastq/ERR194147_2.fastq.gz?_ga=2.134461938.-2019438195.1508521573}.
    \item High coverage Illumina X Ten reads. Raw FASTQ files were downloaded from \url{https://s3-ap-southeast-2.amazonaws.com/kccg-x10-truseq-nano-v2.5-na12878/NA12878_V2.5_Robot_2_R1.fastq.gz} and \url{https://s3-ap-southeast-2.amazonaws.com/kccg-x10-truseq-nano-v2.5-na12878/NA12878_V2.5_Robot_2_R2.fastq.gz}.
\end{enumerate}

\subsubsection{Calling pipelines}

\subsubsection{Evaluation}

\subsection{Germline variants from exome-capture}

\subsection{De novo mutations in parent-offspring trios}

\subsection{Somatic mutations in paired tumour-normal samples}

\subsection{Somatic mutations in tumour only samples}

\subsection{Bacterial calling using HTS data}

\section{Appendix}

\subsection{Variational Bayes}

Given a probability model $p(\boldsymbol{X}, \boldsymbol{Z})$ where $\boldsymbol{X}$ is observed and $\boldsymbol{Z}$ are latent, the true posterior density $p(\boldsymbol{Z} | \boldsymbol{X})$ can be approximated with another distribution $q(\boldsymbol{Z})$ subject to some measure of similarity. A natural choice of similarity is the Kullback–Leibler divergence

\begin{equation}
\label{eq:kl}
   \text{KL} (q\; ||\; p) = -\int q(\boldsymbol{Z}) \ln \frac{p(\boldsymbol{Z} | \boldsymbol{X})}{q(\boldsymbol{Z})} d\boldsymbol{Z}
\end{equation}

This measures the additional amount of information (in nats) required to generate codes from $q$ rather than $p$, it satisfies $\text{KL}(q\; ||\; p) \ge 0$, with equality when $p = q$. So we actually try to minimise this quantity.

We now partition the latent variables $\boldsymbol{Z}$ into a set of disjoint groups denoted by $\boldsymbol{Z_i}$ where $i = 1, \dots, M$ and assume that the $q$ distribution factorises into a product of these groups, i.e.

\begin{equation}
\label{eq:q}
  q(\boldsymbol{Z}) = \prod_{i = 1}^M q_i(\boldsymbol{Z_i})
\end{equation}

This is the only assumption made, in particular the functional form of each $q_i$ is not constrained. The idea is then to optimise each group in tern, which can formally be solved using calculus of variations, but we can to see that by substituting (\ref{eq:q}) into (\ref{eq:kl}) and separating one group $\boldsymbol{Z_j}$ that

\begin{align}
    \text{KL}(q\; ||\; p) &= -\int \prod_{i = 1}^M q_i \left\{ \ln p(\boldsymbol{Z} | \boldsymbol{X}) - \sum_i \ln q_i) \right\} d \boldsymbol{Z}\notag \\
    &= -\int q_j \left\{ \int \ln p(\boldsymbol{Z} | \boldsymbol{X}) \prod_{i \ne j} d \boldsymbol{Z_i} \right\} d \boldsymbol{Z_j} + \int q_j \ln q_j d \boldsymbol{Z_j} + \text{const}\notag\\
    &= -\int q_j \mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})] d \boldsymbol{Z_j} + \int q_j \ln q_j d \boldsymbol{Z_j} + \text{const}\notag\\
    &= \text{KL}(q_j\; ||\; \exp (\mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})])) + \text{const}
\end{align}

Clearly the $q_j$ which minimises this quantity is when $q_j = \exp(\mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})])$ and therefore we find the optimal $q_j$

\begin{equation}
q^*_j(\boldsymbol{Z_j}) = \mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})] + \text{const}
\end{equation}

or equivalently if $p(\boldsymbol{X})$ is absorbed into the constant then

\begin{equation}
\label{eq:q_opt}
q^*_j(\boldsymbol{Z_j}) = \mathbb{E}_{i \ne j}[\ln p(\boldsymbol{X}, \boldsymbol{Z})] + \text{const}
\end{equation}

Note these equations do not represent an explicit solution because they are interdependent. The variational Bayes algorithm therefore proceeds similar to EM by cycling through each group, updating $q^*$, and repeating until convergence. It can be shown that $\text{KL}(q\; ||\; p)$ decreases at each step.

\end{document}
